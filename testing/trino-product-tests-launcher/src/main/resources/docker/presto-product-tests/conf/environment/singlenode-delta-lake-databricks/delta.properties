connector.name=delta_lake
hive.metastore=glue
hive.metastore.glue.region=${ENV:AWS_REGION}
# Keep Hadoop enabled to be able to access dbfs:// locations
fs.hadoop.enabled=true
fs.native-s3.enabled=true
# We need to give access to bucket owner (the AWS account integrated with Databricks), otherwise files won't be readable from Databricks
#hive.s3.upload-acl-type=BUCKET_OWNER_FULL_CONTROL
delta.enable-non-concurrent-writes=true
delta.hive-catalog-name=hive
